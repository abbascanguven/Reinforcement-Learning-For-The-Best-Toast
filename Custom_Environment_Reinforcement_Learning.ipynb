{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class BestToast(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Action we can take, down, up and stay\n",
    "        self.action_space = Discrete(3)\n",
    "        # temperature \n",
    "        self.observation_space = Box(low = np.array([0]), high = np.array([250]))\n",
    "        # set start temperature\n",
    "        self.state = 156 + random.randint(-2,2)\n",
    "        #set cooking time\n",
    "        self.cooking_time = 216\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\"\n",
    "        Action list= \n",
    "        0 -1 = -1 temperature\n",
    "        1 -1 =  0 temperature\n",
    "        2 -1=  +1 temperature\n",
    "        \"\"\"\n",
    "        self.state += action -1\n",
    "\n",
    "        # reduce cooking time by 1 sec\n",
    "        self.cooking_time -= 1\n",
    "\n",
    "        # reward\n",
    "        if self.state >=154 and self.state <= 158:\n",
    "            reward =1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "\n",
    "     # if cooking is done stop\n",
    "        if self.cooking_time <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 156 + random.randint(-2,2)\n",
    "        # Reset shower time\n",
    "        self.shower_length =  216\n",
    "        return self.state\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "env = BestToast()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/can/anaconda3/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation = \"relu\", input_shape = states))\n",
    "    model.add(Dense(24, activation = \"relu\"))\n",
    "    model.add(Dense(actions, activation = \"linear\"))\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = build_model(states, actions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 30000, window_length = 1)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy, nb_actions = actions, nb_steps_warmup = 10, target_model_update = 1e-2)\n",
    "    return dqn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics = [\"mae\"])\n",
    "dqn.fit(env, nb_steps = 30000, visualize = False, verbose = 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/can/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for 30000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 7:37 - reward: 1.0000"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/can/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/home/can/anaconda3/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000/10000 [==============================] - 102s 10ms/step - reward: 0.7894\n",
      "9785 episodes - episode_reward: 0.807 [-1.000, 216.000] - loss: 3.753 - mae: 1.264 - mean_q: 3.122\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: 0.7706\n",
      "10000 episodes - episode_reward: 0.771 [-1.000, 1.000] - loss: 0.182 - mae: 0.598 - mean_q: 1.019\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 0.7636\n",
      "done, took 314.937 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe90a623820>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "scores = dqn.test(env, nb_episodes=20, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing for 20 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "Episode 2: reward: 1.000, steps: 1\n",
      "Episode 3: reward: 1.000, steps: 1\n",
      "Episode 4: reward: 1.000, steps: 1\n",
      "Episode 5: reward: 1.000, steps: 1\n",
      "Episode 6: reward: 1.000, steps: 1\n",
      "Episode 7: reward: 1.000, steps: 1\n",
      "Episode 8: reward: 1.000, steps: 1\n",
      "Episode 9: reward: 1.000, steps: 1\n",
      "Episode 10: reward: 1.000, steps: 1\n",
      "Episode 11: reward: 1.000, steps: 1\n",
      "Episode 12: reward: 1.000, steps: 1\n",
      "Episode 13: reward: 1.000, steps: 1\n",
      "Episode 14: reward: 1.000, steps: 1\n",
      "Episode 15: reward: 1.000, steps: 1\n",
      "Episode 16: reward: 1.000, steps: 1\n",
      "Episode 17: reward: 1.000, steps: 1\n",
      "Episode 18: reward: 1.000, steps: 1\n",
      "Episode 19: reward: 1.000, steps: 1\n",
      "Episode 20: reward: 1.000, steps: 1\n",
      "1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Reference https://github.com/nicknochnack/OpenAI-Reinforcement-Learning-with-Custom-Environment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c5775ed0003054831ab3876049f7b80e194e5174c7ebbac9f9411d6643a0293c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}